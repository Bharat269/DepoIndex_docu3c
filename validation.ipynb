{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4621a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\bhara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bhara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pypdf\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import argparse\n",
    "import google.genai as genai\n",
    "from google.genai import types\n",
    "import json\n",
    "from docx import Document\n",
    "from typing import List, Dict, Tuple \n",
    "import pydantic\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4a294ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: DepostionForPersisYu_LinkPDF.pdf...\n",
      "Done. Created a dictionary with 89 pages.\n",
      "Loading Table of Contents from: toc.json...\n",
      "Done. Loaded 60 topics.\n"
     ]
    }
   ],
   "source": [
    "path = 'DepostionForPersisYu_LinkPDF.pdf'\n",
    "print(f\"Processing PDF: {path}...\")\n",
    "\n",
    "reader = pypdf.PdfReader(path)\n",
    "num_pages = len(reader.pages)\n",
    "page_dict = {}\n",
    "\n",
    "for i in range(num_pages):\n",
    "    end = False\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    \n",
    "    if text is None:\n",
    "        page_dict[i + 1] = \"\" # Use 1-based indexing for pages\n",
    "        continue\n",
    "\n",
    "    # Clean the text\n",
    "    text = text.lower()\n",
    "    if re.search(pattern='witness signature', string=text):\n",
    "        end = True\n",
    "    text = re.sub(pattern=r'\\b\\d{2}:\\d{2}\\b', string=text, repl='')\n",
    "    text = re.sub(pattern=r\"page (\\d+)\", repl='', string=text)\n",
    "    \n",
    "    page_dict[i + 1] = text\n",
    "    if(end):\n",
    "        break\n",
    "\n",
    "print(f\"Done. Created a dictionary with {len(page_dict)} pages.\")\n",
    "\n",
    "print(f\"Loading Table of Contents from: {'toc.json'}...\")\n",
    "\n",
    "try:\n",
    "    with open('toc.json', 'r') as f:\n",
    "        toc_data = json.load(f)\n",
    "    print(f\"Done. Loaded {len(toc_data)} topics.\")\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    print(f\"[!] ERROR: Could not read or parse the JSON file. {e}\")\n",
    "    toc_data = [] # Ensure toc_data exists even on failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbd48a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation...\n",
      "...Validation logic complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting validation...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# A topic is considered valid if at least 50% of its keywords are found.\n",
    "# You can adjust this threshold to be more or less strict.\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "validation_score_pages = []\n",
    "for i, item in enumerate(toc_data):\n",
    "    topic_str = item.get(\"topic\")\n",
    "    page_num = item.get(\"page_start\")\n",
    "\n",
    "    # Skip any malformed entries in the JSON\n",
    "    if not all([topic_str, page_num]):\n",
    "        continue\n",
    "\n",
    "    # Get the text for the specified page\n",
    "    page_text = page_dict.get(page_num)\n",
    "    if page_text is None:\n",
    "        continue\n",
    "\n",
    "    # Extract keywords from the topic string (e.g., \"Analysis of ITT\" -> [\"analysis\", \"itt\"])\n",
    "    tokens = word_tokenize(topic_str.lower())\n",
    "    keywords = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    if not keywords:\n",
    "        continue\n",
    "\n",
    "    # Check for the presence of each keyword on the page\n",
    "    found_count = sum(1 for keyword in keywords if keyword in page_text)\n",
    "    \n",
    "    # Determine if the topic is valid based on the threshold\n",
    "    found_ratio = found_count / len(keywords)\n",
    "    validation_score_pages.append(found_ratio)\n",
    "\n",
    "print(\"...Validation logic complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4efe8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation ratios are: [1.0, 0.5, 0.5, 0.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.0, 0.3333333333333333, 0.4, 0.8, 0.8333333333333334, 1.0, 1.0, 0.8, 0.75, 1.0, 1.0, 1.0, 0.5, 0.75, 1.0, 0.8, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.8, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 0.8, 0.8, 0.8571428571428571, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.75, 0.6, 0.6666666666666666, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]\n",
      "The total found ration is: 79.69047619047619\n",
      "The final validation accuracy is: 90.0\n"
     ]
    }
   ],
   "source": [
    "# --- Final Report ---\n",
    "KEYWORD_MATCH_THRESHOLD = 0.5 \n",
    "print(f\"The validation ratios are: {validation_score_pages}\")\n",
    "total_found_ratio = (sum(validation_score_pages) / len(validation_score_pages)) * 100\n",
    "match_accuracy = (sum(1 for i in validation_score_pages if i>=KEYWORD_MATCH_THRESHOLD)/len(validation_score_pages))*100\n",
    "print(f'The total found ration is: {total_found_ratio}')\n",
    "print(f'The final validation accuracy is: {match_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "811782a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_score_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcce864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc6831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
